{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(0)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÊãºË≤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "\n",
    "##########################\n",
    "# 1) Initialize LLaVA\n",
    "##########################\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True\n",
    ").cuda()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "##########################\n",
    "# 2) Collage creation function\n",
    "##########################\n",
    "def create_collage(image_paths, grid_cols=3):\n",
    "    \"\"\"\n",
    "    Read all images in image_paths and create a collage with 'grid_cols' columns.\n",
    "    Returns a single PIL Image.\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        return None\n",
    "\n",
    "    images = [Image.open(p) for p in image_paths]\n",
    "    num_images = len(images)\n",
    "    rows = math.ceil(num_images / grid_cols)\n",
    "    cols = grid_cols\n",
    "\n",
    "    max_width = max(img.width for img in images)\n",
    "    max_height = max(img.height for img in images)\n",
    "\n",
    "    collage_width = cols * max_width\n",
    "    collage_height = rows * max_height\n",
    "    collage = Image.new(\"RGB\", (collage_width, collage_height), (255, 255, 255))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        x = col * max_width\n",
    "        y = row * max_height\n",
    "        collage.paste(img, (x, y))\n",
    "\n",
    "    return collage\n",
    "\n",
    "\n",
    "##########################\n",
    "# 3) Process a single object folder\n",
    "##########################\n",
    "def process_object_folder(obj_folder_path):\n",
    "    \"\"\"\n",
    "    Collect view_*.png images, create collage, display, and use LLaVA to describe the shape and structure.\n",
    "    \"\"\"\n",
    "    # 1) Collect image paths\n",
    "    image_paths = []\n",
    "    for file_name in os.listdir(obj_folder_path):\n",
    "        if file_name.startswith(\"view_\") and file_name.endswith(\".png\"):\n",
    "            image_paths.append(os.path.join(obj_folder_path, file_name))\n",
    "\n",
    "    image_paths.sort()\n",
    "    if not image_paths:\n",
    "        return\n",
    "\n",
    "    # 2) Create collage image\n",
    "    collage_img = create_collage(image_paths, grid_cols=3)\n",
    "\n",
    "    # 3) Display the image\n",
    "    plt.imshow(collage_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # 4) Prepare conversation prompt (corrected version)\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Ignore how the image is constructed or where it comes from. \"\n",
    "                        \"Do not mention 'the image', 'multiple views', or 'viewpoints'. \"\n",
    "                        \"Describe only the **object itself**, including its shape, structure, and geometry. \"\n",
    "                        \"Focus on parts, form, and layout. Do not mention colors, materials, or how the object is presented.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\"\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    # 5) Encode inputs\n",
    "    inputs = processor(\n",
    "        images=collage_img,\n",
    "        text=prompt,\n",
    "        return_tensors='pt'\n",
    "    ).to(\"cuda\", torch.float16)\n",
    "\n",
    "    # 6) Generate description\n",
    "    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    llava_answer = processor.decode(output[0][2:], skip_special_tokens=True)\n",
    "\n",
    "    # 7) Print result\n",
    "    print(\"\\nüîé LLaVA shape/structure description:\\n\")\n",
    "    print(llava_answer)\n",
    "    print(\"\\n‚úÖ Done.\")\n",
    "\n",
    "\n",
    "##########################\n",
    "# 4) Main function\n",
    "##########################\n",
    "def main():\n",
    "    # Replace this with your own dataset root path\n",
    "    root_dir = \"/home/klooom/cheng/3d_retrival/MHSAN/modelnet40-princeton-3d-object-dataset/rendered_views_12\"\n",
    "\n",
    "    finished = False\n",
    "\n",
    "    for category_name in os.listdir(root_dir):\n",
    "        category_path = os.path.join(root_dir, category_name)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            split_path = os.path.join(category_path, split)\n",
    "            if not os.path.isdir(split_path):\n",
    "                continue\n",
    "\n",
    "            for obj_folder in os.listdir(split_path):\n",
    "                obj_folder_path = os.path.join(split_path, obj_folder)\n",
    "                if not os.path.isdir(obj_folder_path):\n",
    "                    continue\n",
    "\n",
    "                process_object_folder(obj_folder_path)\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "        if finished:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# 1. ÂàùÂßãÂåñ LLaVA Ê®°ÂûãËàáËôïÁêÜÂô®\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ").cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# 2. Âª∫Á´ãÊãºË≤ºÂúñÂáΩÂºè\n",
    "def create_collage(image_paths, grid_cols=3):\n",
    "    images = [Image.open(p) for p in image_paths]\n",
    "    num_images = len(images)\n",
    "    rows = math.ceil(num_images / grid_cols)\n",
    "    cols = grid_cols\n",
    "\n",
    "    max_width = max(img.width for img in images)\n",
    "    max_height = max(img.height for img in images)\n",
    "\n",
    "    collage = Image.new(\"RGB\", (cols * max_width, rows * max_height), (255, 255, 255))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        x = col * max_width\n",
    "        y = row * max_height\n",
    "        collage.paste(img, (x, y))\n",
    "\n",
    "    return collage\n",
    "\n",
    "\n",
    "# 3. ÂñÆ‰∏ÄÁâ©‰ª∂ËôïÁêÜ\n",
    "def process_object_folder(obj_folder_path, category, split, results_file):\n",
    "    image_paths = [\n",
    "        os.path.join(obj_folder_path, f)\n",
    "        for f in sorted(os.listdir(obj_folder_path))\n",
    "        if f.startswith(\"view_\") and f.endswith(\".png\")\n",
    "    ]\n",
    "    if not image_paths:\n",
    "        return\n",
    "\n",
    "    collage_img = create_collage(image_paths, grid_cols=4)\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"You will be shown an image. It contains multiple views of the same single object \"\n",
    "                        f\"from the **{category}** category. Please remember that it is only one object seen from different angles.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Now, based on what you saw, describe the object‚Äôs **shape and structure**. \"\n",
    "                        \"Focus on its geometry, parts, and how they are arranged. \"\n",
    "                        \"Do not mention anything about the image, how many views there are, colors, or materials.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        images=collage_img,\n",
    "        text=prompt,\n",
    "        return_tensors='pt'\n",
    "    ).to(\"cuda\", torch.float16)\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    raw_output = processor.decode(output[0][2:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # ÁßªÈô§ÈñãÈ†≠ÁöÑ prompt ÂÖßÂÆπËàá \"ASSISTANT:\" ÂâçÁ∂¥\n",
    "    if \"ASSISTANT:\" in raw_output:\n",
    "        description = raw_output.split(\"ASSISTANT:\")[1].strip()\n",
    "    else:\n",
    "        description = raw_output  # Ëê¨‰∏ÄÊ≤íÊúâÊ®ôË®òÂ∞±Áõ¥Êé•Áî®Êï¥ÊÆµ\n",
    "\n",
    "    # 1) ÂØ´ÂÖ•ÊØèÂÄãË≥áÊñôÂ§æÁöÑ description.txt\n",
    "    desc_path = os.path.join(obj_folder_path, \"description.txt\")\n",
    "    with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(description)\n",
    "\n",
    "    # 2) ÂØ´ÂÖ• results.jsonl\n",
    "    result = {\n",
    "        \"category\": category,\n",
    "        \"split\": split,\n",
    "        \"object_id\": os.path.basename(obj_folder_path),\n",
    "        \"description\": description\n",
    "    }\n",
    "    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\" {category}/{split}/{os.path.basename(obj_folder_path)}\")\n",
    "\n",
    "\n",
    "# 4. ‰∏ªÊµÅÁ®ãÔºöÈÅçÊ≠∑ÊâÄÊúâË≥áÊñôÂ§æ\n",
    "def main():\n",
    "    root_dir = \"/home/klooom/cheng/3d_retrival/MHSAN/modelnet40-princeton-3d-object-dataset/rendered_views_12\"\n",
    "    results_file = \"results.jsonl\"\n",
    "\n",
    "    # Ëã•Â≠òÂú®ËàäÊ™î,ÂÖàÊ∏ÖÁ©∫\n",
    "    if os.path.exists(results_file):\n",
    "        os.remove(results_file)\n",
    "\n",
    "    for category in sorted(os.listdir(root_dir)):\n",
    "        category_path = os.path.join(root_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            split_path = os.path.join(category_path, split)\n",
    "            if not os.path.isdir(split_path):\n",
    "                continue\n",
    "\n",
    "            for obj_folder in sorted(os.listdir(split_path)):\n",
    "                obj_folder_path = os.path.join(split_path, obj_folder)\n",
    "                if not os.path.isdir(obj_folder_path):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    process_object_folder(obj_folder_path, category, split, results_file)\n",
    "                except Exception as e:\n",
    "                    print(f\" Failed: {category}/{split}/{obj_folder} ‚Äî {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab_from_jsonl(results_file, min_freq=2, max_size=10000):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            description = data.get(\"description\", \"\")\n",
    "            # 1) ÂÅöÁ∞°ÊòìÂàÜË©û \n",
    "            tokens = re.findall(r\"\\w+\", description.lower()) \n",
    "            # 2) Êõ¥Êñ∞Ë©ûÈ†ª\n",
    "            word_counter.update(tokens)\n",
    "\n",
    "    # 3) ÊåâË©ûÈ†ªÊéíÂ∫è\n",
    "    sorted_tokens = sorted(word_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 4) ÈÅéÊøæÁ®ÄÊúâ & vocab ÈôêÂà∂\n",
    "    filtered_tokens = [t for t, c in sorted_tokens if c >= min_freq]\n",
    "    filtered_tokens = filtered_tokens[: (max_size - 2)]  # Áµ¶ÁâπÊÆätokenÁïô‰Ωç\n",
    "\n",
    "    # 5) Âª∫Á´ã token2id\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for token in filtered_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    print(f\"Vocabulary size = {len(vocab)} (including special tokens)\")\n",
    "    return vocab\n",
    "\n",
    "# Áî®Ê≥ïÁØÑ‰æã\n",
    "results_file = \"results.jsonl\"  # ‰Ω†ÁöÑË∑ØÂæë\n",
    "vocab = build_vocab_from_jsonl(results_file, min_freq=2, max_size=10000)\n",
    "\n",
    "# vocab ÊúÉÊòØ‰∏ÄÂÄã dict,ÊØîÂ¶ÇÔºö\n",
    "# {\"<pad>\":0, \"<unk>\":1, \"the\":2, \"structure\":3, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def gather_descriptions(results_file, output_txt):\n",
    "    with open(results_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_txt, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            desc = data.get(\"description\", \"\")\n",
    "           \n",
    "            desc = desc.strip()\n",
    "            if desc:\n",
    "                fout.write(desc + \"\\n\")\n",
    "\n",
    "results_file = \"results.jsonl\"\n",
    "text_corpus = \"descriptions.txt\"\n",
    "gather_descriptions(results_file, text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to tokenizer_model/\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os\n",
    "\n",
    "\n",
    "def train_subword_tokenizer(\n",
    "    text_file,\n",
    "    vocab_size=3000,\n",
    "    output_dir=\"tokenizer_model\",\n",
    "    min_frequency=2\n",
    "):\n",
    "    tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=True,  # Ëã•Êúâ‰∏≠Êñá,ÂèØË¶ñÈúÄÊ±ÇÂàáÊèõ\n",
    "        strip_accents=True,\n",
    "        lowercase=True\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Ë®ìÁ∑¥\n",
    "    tokenizer.train(\n",
    "        files=[text_file],\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        limit_alphabet=1000,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # ÂÑ≤Â≠òÊ™îÊ°à (ÊúÉËº∏Âá∫ vocab.txt Á≠âÊ™îÊ°à)\n",
    "    tokenizer.save_model(output_dir)\n",
    "    print(f\"Tokenizer saved to {output_dir}/\")\n",
    "\n",
    "\n",
    "train_subword_tokenizer(\n",
    "    text_file=text_corpus,\n",
    "    vocab_size=3000,\n",
    "    output_dir=\"tokenizer_model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
